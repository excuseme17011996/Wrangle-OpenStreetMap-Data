{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle OpenStreetMap Data (using MongoDB)\n",
    "## Data Wrangling Project\n",
    "#### Data Analyst Nanodegree (Udacity)\n",
    "##### Project No.-04\n",
    "\n",
    "\n",
    "\n",
    "by Rahul Patra.\n",
    "\n",
    "June 7, 2017.\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This project seeks to apply data munging techniques to first analyse and clean Open Street Map data for a single city and then perform an exploratory analysis of the data after importing it into a MongoDB collection.\n",
    "\n",
    "I have choosen the Open Street Map data for Oxford England as for the last two month I had visited the Oxford. Having visited the city on quite a few occasions, I am confident that there is a lot of interesting information to be discovered through this analysis. And I'm unable to get the data over 50 MB size of the place where I actually live. This is another reason for choosing the unique popular place like Oxford."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this project was acquired from Map Zen. The compressed Oxford, England OSM XML data set can be downloaded by following this metro extract [link](https://s3.amazonaws.com/metro-extracts.mapzen.com/oxford_england.osm.bz2). The OSM file is 5.0 MB compressed and 66.2 MB uncompressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following is needed at the beginning of the notebook to make sure the cells execute ok.\n",
    "import sys\n",
    "sys.path.append('./case_study_files')\n",
    "\n",
    "data_name = \"oxford_england\"\n",
    "OSMFILE = \"{}.osm\".format(data_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems encountered in the map :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you would expect from any crowd-sourced data, there is evidence of inconsistent naming conventions as well as evidence of human error when the data was entered. The two main areas of inconsistency that affect this project is in the names used to describe the type of record in the data and the strings used as values within the data. It isn't feasible to analyse all values in the data, but street names are one type of value where inconsistency in values has a significant impact on the quality of the data. \n",
    "\n",
    "The following analysis attempts to find problems with tag key names along with suggested fixes for the problems. There is a similar analysis and set of recommendations for fixing the values used for street names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems related to Tag Key Names :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of keys in each of the 'problem' categories:\n",
      "{'problemchars': 5, 'upper': 120, 'lower': 143057, 'upper_colon': 7638, 'numbers': 769, 'multiple_colons': 608, 'lower_colon': 47850, 'other': 1}\n",
      "There are 1011 unique tag key names in the data set.\n"
     ]
    }
   ],
   "source": [
    "# Find problems with tag names\n",
    "import tags as tags_processor\n",
    "tag_problems = tags_processor.process_map(OSMFILE)\n",
    "\n",
    "# Additional key categories have been added in an attempt to catch additional key name patterns and minimise \n",
    "# the number of keys that fall within the 'other' category.\n",
    "\n",
    "# The problems are defined as follows:\n",
    "#   \"lower\", for tags that contain only lowercase letters and are valid,\n",
    "#   \"upper\", for tags that contain upercase letters,\n",
    "#   \"lower_colon\", for otherwise valid tags with a colon in their names,\n",
    "#   \"uper_colon\", for tags contain both uppercase strings delimited by a colon,\n",
    "#   \"multiple_colons\", for tags contain more than two colon seperated strings,\n",
    "#   \"numbers\", for tags that contain a digit, and\n",
    "#   \"problemchars\", for tags with problematic characters, and\n",
    "#   \"other\", for other tags that do not fall into the other three categories.\n",
    "print \"The number of keys in each of the 'problem' categories:\"\n",
    "print tag_problems['counts']\n",
    "unique_key_names = tags_processor.unique_tag_keys(OSMFILE)\n",
    "print \"There are {} unique tag key names in the data set.\".format(len(unique_key_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is the tags that fall under 'problemchars' that will be particularly problematic as these tag key names aren't valid key names in MongoDB. Let's see what these tags look like and how they could be fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['leaving for now',\n",
       " 'fee:amount:box_van&minibus',\n",
       " 'note:0.1',\n",
       " 'note:0.2',\n",
       " 'note 2']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_problems['keys']['problemchars']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These issues can be easily fixed by making the following replacements:\n",
    "\n",
    "- Replace ' ' with '\\_'\n",
    "- Replace '&' with '\\_and\\_'\n",
    "- Replace '.' with '\\_'\n",
    "\n",
    "The logic to do this has been added to the case_study_files/data.py so that the key values are tidied when the elements are being shaped.\n",
    "\n",
    "We will also add logic to handle the keys that fall under 'upper' and 'upper_colon'. This can be fixed by ensuring that shape_element uses the lowercase version of the strings.\n",
    "\n",
    "The key names that fall under 'multiple_colons' can be handled by adding additional nesting within the JSON data created by shape_element.\n",
    "\n",
    "Tag key names that fall within the 'numbers' category are useful to know about, but numbers are valid characters in MongoDB strings so no additional processing is required.\n",
    "\n",
    "It is also worth looking at the one remaining key that fall under 'other' to see why it doesn't fall within one of the other categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name:sr-Latn'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(tag_problems['keys']['other'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the tag key names that contain numbers, the hyphen is also a valid character in a MongoDB key as this one one 'other' key name is fine as it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems related to street name :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {'1': {'Avenue 1'},\n",
       "             '2': {'Avenue 2'},\n",
       "             '3': {'Avenue 3'},\n",
       "             '4': {'Avenue 4'},\n",
       "             \"Aldate's\": {\"St Aldate's\"},\n",
       "             'Ave': {'Waverly Ave'},\n",
       "             'Barr': {'Upper Barr'},\n",
       "             'Bridge': {'Folly Bridge'},\n",
       "             'Broadway': {'The Broadway'},\n",
       "             'Buildings': {'Manor Buildings'},\n",
       "             'Castle': {'Oxford Castle'},\n",
       "             'Centre': {'Fairfax Centre'},\n",
       "             'Centremead': {'Centremead'},\n",
       "             'Chorefields': {'Chorefields'},\n",
       "             'Clements': {'St Clements'},\n",
       "             'Cloisters': {'Temple Cloisters'},\n",
       "             'Close': {'Acland Close',\n",
       "              'Acre Close',\n",
       "              'Amory Close',\n",
       "              'Ashcroft Close',\n",
       "              'Ashroft Close',\n",
       "              'Barn Close',\n",
       "              'Bartlemas Close',\n",
       "              'Benouville Close',\n",
       "              'Blackburn Close',\n",
       "              'Broad Close',\n",
       "              'Browns Close',\n",
       "              'Bullstake Close',\n",
       "              'Burgan Close',\n",
       "              'Burrows Close',\n",
       "              'Bushy Close',\n",
       "              'Cardinal Close',\n",
       "              'Carey Close',\n",
       "              'Cholsey Close',\n",
       "              'Clover Close',\n",
       "              'Compass Close',\n",
       "              'Complins Close',\n",
       "              'Coolidge Close',\n",
       "              'Cornwallis Close',\n",
       "              'Denton Close',\n",
       "              'Don Bosco Close',\n",
       "              'Dora Carr Close',\n",
       "              'Dover Close',\n",
       "              'East Field Close',\n",
       "              'Egrove Close',\n",
       "              'Eleanor Close',\n",
       "              'Evelyn Close',\n",
       "              'Galpin Close',\n",
       "              'Goodey Close',\n",
       "              'Halls Close',\n",
       "              'Hayday Close',\n",
       "              'Highbank Close',\n",
       "              'Hillsborough Close',\n",
       "              'Homestall Close',\n",
       "              'Hundred Acres Close',\n",
       "              'Hunter Close',\n",
       "              'Hutchcomb Farm Close',\n",
       "              'Ivy Close',\n",
       "              'John Parker Close',\n",
       "              'Kames Close',\n",
       "              'Kennedy Close',\n",
       "              'Lambton Close',\n",
       "              'Long Close',\n",
       "              'Marley Close',\n",
       "              'Meyseys Close',\n",
       "              'Nobles Close',\n",
       "              'Osborne Close',\n",
       "              'Owlington Close',\n",
       "              'Pottle Close',\n",
       "              'Pulker Close',\n",
       "              'Queens Close',\n",
       "              'Riely Close',\n",
       "              'Robinson Close',\n",
       "              'Roundham Close',\n",
       "              'Silkdale Close',\n",
       "              'Skene Close',\n",
       "              'Songers Close',\n",
       "              'South Close',\n",
       "              \"St Ebba's Close\",\n",
       "              \"Stimpson's Close\",\n",
       "              'Stone Close',\n",
       "              'Stubble Close',\n",
       "              'Troy Close',\n",
       "              'Venneit Close',\n",
       "              'Yeats Close'},\n",
       "             'Crescent': {'Brookfield Crescent',\n",
       "              'Canning Crescent',\n",
       "              'Corunna Crescent',\n",
       "              'Fox Crescent',\n",
       "              'Herschel Crescent',\n",
       "              'Hugh Allen Crescent',\n",
       "              'Kendall Crescent',\n",
       "              'Kersington Crescent',\n",
       "              'Lockheart Crescent',\n",
       "              'Normandy Crescent',\n",
       "              'Salisbury Crescent',\n",
       "              'Walton Crescent',\n",
       "              'Westbury Crescent',\n",
       "              'Wykeham Crescent'},\n",
       "             'Down': {'Colegrove Down'},\n",
       "             'Driftway': {'Brasenose Driftway', 'Horspath Driftway'},\n",
       "             'East': {'Woodstock Road East'},\n",
       "             'Entry': {'Friars Entry'},\n",
       "             'Furze': {'Demesne Furze', 'Inott Furze', 'Town Furze'},\n",
       "             'Gardens': {'Court Place Gardens',\n",
       "              'Croxford Gardens',\n",
       "              'Mileway Gardens',\n",
       "              'Norham Gardens'},\n",
       "             'Giles': {'St Giles'},\n",
       "             \"Giles'\": {\"Saint Giles'\", \"St Giles'\"},\n",
       "             'Glebe': {'The Glebe'},\n",
       "             'Glebelands': {'Glebelands'},\n",
       "             'Grates': {'The Grates'},\n",
       "             'Green': {'Gloucester Green'},\n",
       "             'Ground': {\"Cox's Ground\"},\n",
       "             'Hill': {'Cumnior Hill',\n",
       "              'Cumnor Hill',\n",
       "              'Heyford Hill',\n",
       "              'Rose Hill',\n",
       "              'The Cedars, Cumnor Hill',\n",
       "              'Tumbledown Hill'},\n",
       "             'Hillside': {'Hillside'},\n",
       "             'Hollow': {'Quarry Hollow'},\n",
       "             'House': {'Salesian House'},\n",
       "             'Hurdeswell': {'Hurdeswell'},\n",
       "             'Mead': {'Osney Mead'},\n",
       "             'Meadow': {'Stone Meadow', 'Upper Meadow'},\n",
       "             'Mews': {'Fairfax Mews', 'Temple Mews'},\n",
       "             'Moors': {'Peat Moors'},\n",
       "             'Parade': {'Elms Parade', 'South Parade', 'The Parade'},\n",
       "             'Park': {'Seacourt Tower Retail Park',\n",
       "              'Southfield Park',\n",
       "              'Templers Shopping Park',\n",
       "              'The Park'},\n",
       "             'Path': {'The Towing Path'},\n",
       "             'Phelps': {'The Phelps'},\n",
       "             'Plain': {'The Plain'},\n",
       "             'Quarter': {'Oxford Castle Quarter'},\n",
       "             'Quorum': {'The Quorum'},\n",
       "             'Rd': {'Abingdon Rd',\n",
       "              'Appleton Rd',\n",
       "              'Faringdon Rd',\n",
       "              'Fogwell Rd',\n",
       "              'Glebe Rd',\n",
       "              'Oxford Rd'},\n",
       "             'Ridgeway': {'The Ridgeway'},\n",
       "             'Rise': {'Third Acre Rise'},\n",
       "             'Roundabout': {'Wolvercote Roundabout'},\n",
       "             'Roundway': {'The Roundway'},\n",
       "             'Row': {'Hollybush Row'},\n",
       "             'Slade': {'The Slade'},\n",
       "             'St': {'High St'},\n",
       "             'Sunnyside': {'Sunnyside'},\n",
       "             'Terrace': {'Cranham Terrace', 'Weymann Terrace'},\n",
       "             'Town': {'Park Town'},\n",
       "             'Trees': {'Cherry Trees'},\n",
       "             'Turn': {'Iffley Turn'},\n",
       "             'Valley': {'Lye Valley'},\n",
       "             'View': {'Fair View'},\n",
       "             'Village': {'Wootton Village'},\n",
       "             'Walk': {'Church Walk'},\n",
       "             'Way': {'Alec Issigonis Way',\n",
       "              \"Arnold's Way\",\n",
       "              'Arnolds Way',\n",
       "              'Ashhurst Way',\n",
       "              'Bellenger Way',\n",
       "              'Church Way',\n",
       "              'Delamare Way',\n",
       "              'Dunnock Way',\n",
       "              'Elizabeth Jennings Way',\n",
       "              'Gordon Woodward Way',\n",
       "              'Headley Way',\n",
       "              'Hollow Way',\n",
       "              'Leander Way',\n",
       "              'Manzil Way',\n",
       "              'Mazil Way',\n",
       "              'Middle Way',\n",
       "              'Navigation Way',\n",
       "              'Oakwood Way',\n",
       "              'Orchard Way',\n",
       "              'Pinnocks Way',\n",
       "              'Reliance Way',\n",
       "              'Reliuance Way',\n",
       "              'Seven Sisters Way',\n",
       "              'Sheldon Way',\n",
       "              'Unit 1, Seacourt Towers, West Way',\n",
       "              'West Way',\n",
       "              'Wilsdon Way'},\n",
       "             'Way,': {'Roger Dudman Way,'},\n",
       "             'Way?': {'Reliance Way?'},\n",
       "             'West': {'Sandy Lane West'},\n",
       "             'Winnyards': {'The Winnyards'},\n",
       "             'Woodfield': {'Woodfield'},\n",
       "             'Yard': {'New Inn Yard'},\n",
       "             'road': {'Marston road'},\n",
       "             'way': {'Reliance way'}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find problems with street names\n",
    "import audit as street_name_auditor\n",
    "street_name_auditor.audit(OSMFILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The large majority of these street names are valid, but there are a few problems. There are a few cases of abbreviated street names such as 'Ave', 'Rd' and 'St'. There are also several cases of punctuations mistakes such as 'Way?' and 'Way,'. There are cases of both 'road' and 'way' being lowercase. There is also a typo where 'Reliuance Way' should be 'Reliance Way'. The oddest problems are the cases of 'Avenue 1' through 'Avenue 4'. In Kennington just outside of Oxford the main street is called 'The Avenue'. I believe these street names may be mistakes or parsing issues. They don't match any obvious street names and as a result will be treated as anomalies and ignored.\n",
    "\n",
    "Logic has been added to case_study_files/data.py to clean up these inconsistencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems related with Food :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of food nodes: 523\n",
      "Number of food nodes with a cuisine and amenity: 213\n",
      "Number of food nodes without a cuisine: 303\n",
      "Number of food nodes without an amenity: 7\n"
     ]
    }
   ],
   "source": [
    "import cuisine as cuisine_auditor\n",
    "food_nodes = cuisine_auditor.audit(OSMFILE)\n",
    "\n",
    "food_nodes_with_cuisine_and_amenity = [n for n in food_nodes if 'cuisine' in n and 'amenity' in n]\n",
    "food_nodes_without_cuisine = [n for n in food_nodes if 'cuisine' not in n]\n",
    "food_nodes_without_amenity = [n for n in food_nodes if 'amenity' not in n]\n",
    "print \"Number of food nodes: {}\".format(len(food_nodes))\n",
    "print \"Number of food nodes with a cuisine and amenity: {}\".format(len(food_nodes_with_cuisine_and_amenity))\n",
    "print \"Number of food nodes without a cuisine: {}\".format(len(food_nodes_without_cuisine))\n",
    "print \"Number of food nodes without an amenity: {}\".format(len(food_nodes_without_amenity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amenity counts for food nodes without a cuisine:\n",
      "Counter({'pub': 152, 'cafe': 73, 'restaurant': 31, 'fast_food': 26, 'bar': 21})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "amenities = []\n",
    "for node in food_nodes_without_cuisine:\n",
    "    amenities.append(node['amenity'])\n",
    "\n",
    "print \"Amenity counts for food nodes without a cuisine:\"\n",
    "print Counter(amenities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cuisine': 'sandwich', 'name': \"Bunny's\"},\n",
       " {'cuisine': 'chinese', 'name': 'Chopsticks Chinese Restaurant'},\n",
       " {'cuisine': 'greek', 'name': 'Meli Deli'},\n",
       " {'cuisine': 'donut', 'name': 'Krispy Kreme'},\n",
       " {'cuisine': 'polish', 'name': 'Euro Foods'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reasonable_anomoly(n):\n",
    "    return 'name' in n and ('shop' in n or 'disused' in n)\n",
    "[{'name': n['name'], 'cuisine': n['cuisine']} for n in food_nodes_without_amenity if reasonable_anomoly(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuisine counts for restaurant nodes:\n",
      "Counter({'indian': 17, 'chinese': 12, 'italian': 9, 'thai': 6, 'burger': 4, 'pizza': 4, 'french': 4, 'tapas': 4, 'asian': 3, 'nepalese': 3, 'sandwich': 3, 'lebanese': 3, 'sushi': 2, 'japanese': 2, 'turkish': 2, 'mexican': 1, 'fish': 1, 'japanese;sushi': 1, 'chinese;indian;thai;halal': 1, 'italian;pizza': 1, 'chicken': 1, 'indian;curry': 1, 'sausage': 1, 'international': 1, 'korean': 1, 'asian;noodle;soup': 1, 'english': 1, 'pizza;pasta': 1, 'crepe': 1, 'pizza,burger': 1, 'greek': 1, 'fish_and_chips': 1, 'Nepalese': 1, 'curry': 1, 'seafood': 1})\n",
      "\n",
      "\n",
      "Cuisine counts for cafe nodes:\n",
      "Counter({'coffee_shop': 15, 'sandwich': 10, 'ice_cream': 4, 'portuguese': 1, 'bubbletea': 1, 'pie': 1, 'bangladeshi': 1})\n",
      "\n",
      "\n",
      "Cuisine counts for pub nodes:\n",
      "Counter({'burger': 2, 'vegetarian': 1, 'pizza': 1, 'american': 1, 'Fish and Chips and other pub favourites': 1})\n",
      "\n",
      "\n",
      "Cuisine counts for bar nodes:\n",
      "Counter({'jamaican': 1, 'drinks': 1})\n",
      "\n",
      "\n",
      "Cuisine counts for fast_food nodes:\n",
      "Counter({'chinese': 17, 'sandwich': 12, 'fish_and_chips': 10, 'pizza': 6, 'burger': 5, 'kebab': 3, 'indian': 3, 'mexican': 2, 'kebab;middle_east': 2, 'asian': 2, 'chicken': 2, 'sushi': 1, 'japanese': 1, 'burgers;snacks;full_english': 1, 'cornish_pasty': 1, 'persian': 1, 'moroccan': 1, 'crepe': 1, 'burger;sausage': 1, 'italian': 1})\n",
      "\n",
      "\n",
      "Cuisine counts for delicatessen nodes:\n",
      "Counter({'brazillian;portuguese': 1})\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for amenity in cuisine_auditor.food_amenities:\n",
    "    cuisines = []\n",
    "    relevant_nodes = [n for n in food_nodes_with_cuisine_and_amenity if n['amenity'] == amenity]\n",
    "    for node in relevant_nodes:\n",
    "        cuisines.append(node['cuisine'])\n",
    "    print \"Cuisine counts for {} nodes:\".format(amenity)\n",
    "    print Counter(cuisines)\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject name: The Tree   Subject amenity: pub \n",
      "\n",
      "Similarity score: 0.538   Name: The Gardeners Arms   Cuisine: vegetarian\n",
      "Similarity score: 0.526   Name: White Horse   Cuisine: Fish and Chips and other pub favourites\n",
      "Similarity score: 0.500   Name: The White Rabbit   Cuisine: pizza\n",
      "Similarity score: 0.500   Name: The Chequers   Cuisine: burger\n",
      "Similarity score: 0.462   Name: The Head of the River   Cuisine: burger\n"
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def similarity_by_name(a, b):\n",
    "    if 'name' in a and 'name' in b:\n",
    "        a = a['name'].replace('the', '').lower()\n",
    "        b = b['name'].replace('the', '').lower()\n",
    "        return SequenceMatcher(None, a, b).ratio()\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "subject = food_nodes_without_cuisine[0]\n",
    "\n",
    "processed_nodes = []\n",
    "food_nodes_with_same_amenity = [n for n in food_nodes_with_cuisine_and_amenity if n['amenity'] == subject['amenity']]\n",
    "for node in food_nodes_with_same_amenity:\n",
    "    if 'name' in node:\n",
    "        processed_nodes.append({'similarity': similarity_by_name(subject, node), 'node': node})\n",
    "\n",
    "print \"Subject name: {}   Subject amenity: {} \\n\".format(subject['name'], subject['amenity'])\n",
    "sorted_results = sorted(processed_nodes, key=lambda k: k['similarity'], reverse=True)\n",
    "for result in sorted_results[:5]:\n",
    "    node = result['node']\n",
    "    score = '%.3f' % result['similarity']\n",
    "    print \"Similarity score: {}   Name: {}   Cuisine: {}\".format(score, node['name'], node['cuisine'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analysis shows that there are definite problems with the cuisine classifications of food nodes within the data. Of the 523 nodes analysed, 303 have amenity types but no cuisine type and 7 have a cuisine type but no amenity type. The 7 nodes that have a cuisine but are missing an amenity can be explained as follows and don't need further analysis: four are shops and don't require an amenity and two are marked as disused one of which is missing. Only 'The Oriental Condor' looks like it should have an amenity type but does not.\n",
    "\n",
    "The 303 nodes that have food related amenities but lack a cuisine are broken down by amenity as follows:\n",
    "\n",
    "- pub: 152 \n",
    "- cafe: 73\n",
    "- restaurant: 31 \n",
    "- fast_food: 26\n",
    "- bar: 21\n",
    "\n",
    "I believe that the disproportionate representation of pubs within this data can be explained by either the pubs not serving food and as a result not needing a cuisine type although I find it hard to believe that Oxford has 153 pubs none of which serve food. What may be more likely is that the creators of this data assumed that a node marked as a 'pub' serves 'pub food' and as a result does not need a 'cuisine'.\n",
    "\n",
    "An analysis of the cuisines for each of the remaining 213 nodes broken down by amenity does not show a consistent pattern. A winner take all strategy (majority cuisine by amenity) to fix the 303 problem nodes would mean assigning 'indian' to all 'restaurants' and 'chinese' to all 'fast_food' nodes. This hardly seems appropriate.\n",
    "\n",
    "A more sophisticated strategy would be to do a similarity analysis between node names and see if appropriate cuisines could be inferred from the node names. An initial attempt at fixing the 303 problem nodes this way also proved inappropriate. A sample application of the approach resulted in 'The Tree' pub being considered most similar to 'The Gardeners Arms' pub. This is a clever result in terms of string similarity, but 'The Gardeners Arms' servers vegetarian food and a little bit of research shows that the pub at The Tree Hotel serves standard pub fair (curry, steak etc.). This illustrates why trying to infer the cuisine simply by doing a string similarity analysis on the node names will likely result in badly assigned cuisine types. For this reason, I decided not to attempt to fix the cuisine problems deciding that adding bad information to the data is worse than accepting that some information is missing.\n",
    "\n",
    "If fixing these problems was imperative, I would attempt to train a supervised learning algorithm to try and harness more information than just the names in attempt to more accurately label the missing cuisine types. If I were to apply this strategy, I would look for more data than just the 213 nodes from the Oxford data set as it would be unlikely that out of 516 nodes, 213 labeled data would be able to generalise over the remaining 303."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in the oxford_england MongoDB collection: 321322\n",
      "A sample record from the oxford_england MongoDB collection: {u'created': {u'changeset': u'10706805', u'version': u'4', u'uid': u'27408', u'timestamp': u'2012-02-16T23:08:31Z', u'user': u'Andrew Chadwick'}, u'pos': [51.6994959, -1.2645627], u'visible': None, u'_id': ObjectId('57963746d0958625140ac407'), u'type': u'node', u'id': u'194502'}\n"
     ]
    }
   ],
   "source": [
    "# This snippet uses the process_map function from the case study data.py script. This process is idempotent and will \n",
    "# only load more data into MongoDB if any of the following happens:\n",
    "#     - The number of records in the MongoDB collection matching the data_name variable has changed.\n",
    "#     - The value of the data_name variable changes.\n",
    "#     - The number of records in the OSM file matching data_name changes.\n",
    "\n",
    "import data as data_processor\n",
    "data = data_processor.process_map(OSMFILE, True)\n",
    "\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient(\"mongodb://localhost:27017\")\n",
    "db = client.oxford_england_sample\n",
    "collection = getattr(db, data_name)\n",
    "\n",
    "collection_count = collection.count()\n",
    "\n",
    "# If the collection size dowsn't match the data size, load/reload the data.\n",
    "if collection_count != len(data):\n",
    "    if collection_count > 0:\n",
    "        collection.drop()\n",
    "    collection.insert_many(data)\n",
    "\n",
    "collection_count = collection.count()\n",
    "sample_record = collection.find_one()\n",
    "print \"Number of records in the {} MongoDB collection: {}\".format(data_name, collection_count)\n",
    "print \"A sample record from the {} MongoDB collection: {}\".format(data_name, sample_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oxford_england.osm is 66.214418 MB in size.\n",
      "oxford_england.osm.json is 98.967713 MB in size.\n"
     ]
    }
   ],
   "source": [
    "# Size of the OSM and JSON files.\n",
    "import os\n",
    "def get_size_in_mb_of_relative_file(file_name):\n",
    "    wd = %pwd\n",
    "    return os.stat(wd + '/' + file_name).st_size / 1000.0 / 1000.0\n",
    "\n",
    "for file_name in [OSMFILE, OSMFILE + '.json']:\n",
    "    file_size = get_size_in_mb_of_relative_file(file_name)\n",
    "    print \"{} is {} MB in size.\".format(file_name, file_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in the oxford_england MongoDB collection: 321322\n"
     ]
    }
   ],
   "source": [
    "# Number of records:\n",
    "print \"Number of records in the {} MongoDB collection: {}\".format(data_name, collection_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'node' records in the oxford_england MongoDB collection: 274664\n"
     ]
    }
   ],
   "source": [
    "# Number of nodes:\n",
    "num_nodes = collection.find({\"type\":\"node\"}).count()\n",
    "print \"Number of 'node' records in the {} MongoDB collection: {}\".format(data_name, num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'way' records in the oxford_england MongoDB collection: 46600\n"
     ]
    }
   ],
   "source": [
    "# Number of ways:\n",
    "num_ways = collection.find({\"type\":\"way\"}).count()\n",
    "print \"Number of 'way' records in the {} MongoDB collection: {}\".format(data_name, num_ways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique 'users' in the oxford_england MongoDB collection: 567\n"
     ]
    }
   ],
   "source": [
    "# Number of unique users:\n",
    "num_unique_users = len(collection.distinct(\"created.user\"))\n",
    "print \"Number of unique 'users' in the {} MongoDB collection: {}\".format(data_name, num_unique_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique 'amenity' values in the oxford_england MongoDB collection: 127\n"
     ]
    }
   ],
   "source": [
    "# Number of unique amenity types:\n",
    "num_unique_amenities = len(collection.distinct(\"amenity\"))\n",
    "print \"Number of unique 'amenity' values in the {} MongoDB collection: {}\".format(data_name, num_unique_amenities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique 'cuisine' values in the oxford_england MongoDB collection: 57\n"
     ]
    }
   ],
   "source": [
    "# Number of unique cuisine types:\n",
    "num_unique_cuisines = len(collection.distinct(\"cuisine\"))\n",
    "print \"Number of unique 'cuisine' values in the {} MongoDB collection: {}\".format(data_name, num_unique_cuisines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'university' and 'college' records in the oxford_england MongoDB collection: 119\n"
     ]
    }
   ],
   "source": [
    "# Number of universities and colleges:\n",
    "num_colleges = collection.find({\"amenity\": {\"$in\": [\"university\", \"college\"]}}).count()\n",
    "print \"Number of 'university' and 'college' records in the {} MongoDB collection: {}\".format(data_name, num_colleges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other interesting facts about the datasets :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oxford is a very interesting city not simply because of the history and prestige of Oxford University, but also because of its density and diversity of amenities woven within a fabric of well integrated network of pedestrian, bicycle, and public transport routes. It would be very interesting to explore the relationships between these amenities and the various networks that allow people to move around the city.\n",
    "\n",
    "The outcome of analysis could be very helpful in planning activities and events within the city as well as providing information about which parts of the city are likely to be busiest during the tourist season.\n",
    "\n",
    "That said, the OSM data for Oxford may not make such an analysis very easy to undertake. For example, data about the bus and rail networks are encoded using the NAPTAN schema ([NAPTAN stands for National Public Transport Access Nodes](https://www.gov.uk/government/publications/national-public-transport-access-node-schema)) but this information does not always contain information about street names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A bus stop node without street information.\n",
      "{u'direction': u'W', u'name': u'Herschel Crescent', u'journeys': u'6', u'created': {u'changeset': u'8007524', u'version': u'6', u'uid': u'74570', u'timestamp': u'2011-04-29T22:38:18Z', u'user': u'Richard Mann'}, u'pos': [51.7249536, -1.2152904], u'visible': None, u'frequency': u'0', u'naptan': {u'Bearing': u'W'}, u'source': u'photograph', u'bus_routes': u'16A', u'_id': ObjectId('57963746d0958625140add4d'), u'type': u'node', u'id': u'16640101', u'highway': u'bus_stop'}\n",
      "\n",
      "\n",
      "A bus stop node with street information.\n",
      "{u'direction': u'W', u'name': u'Long Lane', u'journeys': u'6', u'created': {u'changeset': u'8007524', u'version': u'6', u'uid': u'74570', u'timestamp': u'2011-04-29T22:38:18Z', u'user': u'Richard Mann'}, u'pos': [51.7249171, -1.2181715], u'note': u'NAPTAN node in wrong place, and erroneously marked CUS', u'visible': None, u'frequency': u'0', u'naptan': {u'Bearing': u'W', u'Indicator': u'Opp Sheldon Way', u'BusStopType': u'CUS', u'CommonName': u'Long Lane', u'PlusbusZoneRef': u'OXFD', u'Street': u'Long Lane', u'Landmark': u'Sheldon Way', u'AtcoCode': u'340000958OPP', u'NaptanCode': u'oxfatjdj'}, u'source': u'naptan_import;photograph', u'bus_routes': u'16A', u'_id': ObjectId('57963746d0958625140add54'), u'type': u'node', u'id': u'16640114', u'highway': u'bus_stop'}\n"
     ]
    }
   ],
   "source": [
    "print \"A bus stop node without street information.\"\n",
    "print collection.find_one({\"naptan\": {\"$exists\": 1}, \"highway\": \"bus_stop\"})\n",
    "print \"\\n\"\n",
    "print \"A bus stop node with street information.\"\n",
    "print collection.find_one({\"naptan.Street\": {\"$exists\": 1}, \"highway\": \"bus_stop\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The top 10 most common amenities :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'count': 741, u'_id': u'parking'}\n",
      "{u'count': 521, u'_id': u'bicycle_parking'}\n",
      "{u'count': 332, u'_id': u'post_box'}\n",
      "{u'count': 187, u'_id': u'bench'}\n",
      "{u'count': 164, u'_id': u'pub'}\n",
      "{u'count': 163, u'_id': u'place_of_worship'}\n",
      "{u'count': 142, u'_id': u'telephone'}\n",
      "{u'count': 130, u'_id': u'restaurant'}\n",
      "{u'count': 110, u'_id': u'cafe'}\n",
      "{u'count': 102, u'_id': u'school'}\n"
     ]
    }
   ],
   "source": [
    "results = collection.aggregate([\n",
    "        {\"$match\": {\"amenity\": {\"$exists\": 1}}}, \n",
    "        {\"$group\": {\"_id\": \"$amenity\", \"count\": {\"$sum\": 1}}}, \n",
    "        {\"$sort\": {\"count\": -1}}, \n",
    "        {\"$limit\": 10}\n",
    "    ])\n",
    "for result in results:\n",
    "    print result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The top 10 most common types of cuisine :\n",
    "\n",
    "For food nodes where the cuisine is known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'count': 31, u'_id': u'chinese'}\n",
      "{u'count': 26, u'_id': u'sandwich'}\n",
      "{u'count': 20, u'_id': u'indian'}\n",
      "{u'count': 15, u'_id': u'coffee_shop'}\n",
      "{u'count': 11, u'_id': u'fish_and_chips'}\n",
      "{u'count': 11, u'_id': u'burger'}\n",
      "{u'count': 11, u'_id': u'pizza'}\n",
      "{u'count': 10, u'_id': u'italian'}\n",
      "{u'count': 6, u'_id': u'thai'}\n",
      "{u'count': 5, u'_id': u'asian'}\n"
     ]
    }
   ],
   "source": [
    "results = collection.aggregate([\n",
    "        {\"$match\": {\"cuisine\": {\"$exists\": 1}}}, \n",
    "        {\"$group\": {\"_id\": \"$cuisine\", \"count\": {\"$sum\": 1}}}, \n",
    "        {\"$sort\": {\"count\": -1}}, \n",
    "        {\"$limit\": 10}\n",
    "    ])\n",
    "for result in results:\n",
    "    print result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 most commonly found places to eat :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count 6: The Red Lion (pub - unknown cuisine)\n",
      "Count 5: La Croissanterie (cafe - unknown cuisine)\n",
      "Count 5: Taylors (restaurant - sandwich)\n",
      "Count 4: The White Hart (pub - unknown cuisine)\n",
      "Count 3: Mission Burrito (fast_food - mexican)\n",
      "Count 3: McDonald's (fast_food - burger)\n",
      "Count 3: Subway (fast_food - sandwich)\n",
      "Count 3: Pizza Hut (restaurant - pizza)\n",
      "Count 3: Mortons (cafe - sandwich)\n",
      "Count 3: Costa Coffee (cafe - unknown cuisine)\n"
     ]
    }
   ],
   "source": [
    "results = collection.aggregate([\n",
    "        {\"$match\": {\"amenity\": {\"$in\": [\"restaurant\", \"cafe\", \"pub\", \"bar\", \"fast_food\", \"delicatessen\"]}}}, \n",
    "        {\"$match\": {\"name\": {\"$exists\": 1}}},\n",
    "        {\"$group\": {\n",
    "                \"_id\": \"$name\", \n",
    "                \"amenity\": {\"$first\": \"$amenity\"}, \n",
    "                \"cuisine\": {\"$push\": \"$cuisine\"}, \n",
    "                \"count\": {\"$sum\": 1}\n",
    "            }},\n",
    "        {\"$project\": {\n",
    "                \"_id\": 0, \n",
    "                \"count\": 1, \n",
    "                \"name\": \"$_id\", \n",
    "                \"type\" : {\"$concat\": [\n",
    "                        \"$amenity\", \" - \", \n",
    "                        {\"$ifNull\": [{\"$arrayElemAt\": [\"$cuisine\", 0 ]}, \"unknown cuisine\"] }\n",
    "                    ]}\n",
    "            }},\n",
    "        {\"$sort\": {\"count\": -1}}, \n",
    "        {\"$limit\": 10}\n",
    "    ])\n",
    "\n",
    "for result in results:\n",
    "    print \"Count {}: {} ({})\".format(result['count'], result['name'], result['type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to note that Asian cuisines occupy 4 of the top top types of cuisine (1: Chinese, 3: Indian, 9: Thai, 1: Asian), yet there isn't a single business offering Asian cuisine in the top 10 most commonly found places to eat. This begins to illustrate the trend that even though business offering Asian cuisine are numerous, they tend to be independent establishments rather than franchises or chains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The total reported bicycle parking capacity :\n",
    "\n",
    "We can't make any assumptions for records where 'capacity' is known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'_id': u'total_bicycle_parking_capacity', u'value': 8756.0}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bson.code import Code\n",
    "\n",
    "result = collection.inline_map_reduce(\n",
    "    Code(\"function() {\"\n",
    "         \"    if (this.capacity) {\"\n",
    "         \"        emit('total_bicycle_parking_capacity', Number(this.capacity));\"\n",
    "         \"    }\"\n",
    "         \"}\"), \n",
    "    Code(\"function(key, values) {\"\n",
    "         \"    var total = 0;\"\n",
    "         \"    for (var i = 0; i < values.length; i++) {\"\n",
    "         \"        total += values[i];\"\n",
    "         \"    }\"\n",
    "         \"    return total;\"\n",
    "         \"}\"),\n",
    "    query = {\"amenity\": \"bicycle_parking\"});\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 5 streets with the most bus stops :\n",
    "\n",
    "For bus stop nodes where the street name is known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'street_name': u'Oxford Road', u'num_bus_stops': 43}\n",
      "{u'street_name': u'Banbury Road', u'num_bus_stops': 40}\n",
      "{u'street_name': u'Woodstock Road', u'num_bus_stops': 33}\n",
      "{u'street_name': u'High Street', u'num_bus_stops': 23}\n",
      "{u'street_name': u'Cowley Road', u'num_bus_stops': 20}\n"
     ]
    }
   ],
   "source": [
    "results = collection.aggregate([\n",
    "        {\"$match\": {\"naptan\": {\"$exists\": 1}, \"highway\": \"bus_stop\"}},\n",
    "        {\"$group\": {\"_id\": \"$naptan.Street\", \"num_bus_stops\": {\"$sum\": 1}}},\n",
    "        {\"$project\": {\"_id\": 0, \"num_bus_stops\": 1, \"street_name\": \"$_id\"}},\n",
    "        {\"$sort\": {\"num_bus_stops\": -1}}, \n",
    "        {\"$limit\": 5}\n",
    "    ])\n",
    "for result in results:\n",
    "    print result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I am pleasantly surprised to see just how much information about Oxford is encoded within the OSM data. That said, the inevitable nature of crowd sourced information is clearly present. There is quite a lot of inconsistency in the data (not all illustrated in this project) as well as records that do little more than act as meta information about the creation of the data itself (such as 'fixme' and 'note')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 78 'meta' tag key names making up 7.72% of all unique tag key names.\n",
      "\n",
      "\n",
      "['note2', 'Fixme:responce_2', 'frequency:note', 'note:17', 'note:14', 'note:15', 'note:13', 'note:10', 'note:11', 'notes', 'oneway:note', 'building:levels:underground:note', 'continental_geometry:note', 'parking:fixme', 'FIXME:nsl', 'note:highway', 'parking:note', 'fixme:lane', 'Note:2', 'maxspeed:note', 'FIXME', 'note:bicycle', 'note:layer', 'Fixme:2', 'fixme:responce_3', 'fixme:responce_5', 'fixme:responce_4', 'note:alt_name', 'fixme', 'note:designation', 'note:cont', 'note:crossing', 'name:note', 'highway:note', 'naptan:Notes', 'FIXME:responce', 'FIXME:cont', 'note:reply', 'note:name', 'note', 'layer:note', 'note:16', 'note:barrier', 'not:name:note', 'bicycle:note', 'note:geometry', 'note:frequency', 'note:surface', 'alignment_note', 'note:park_and_ride', 'note:0.1', 'note:0.2', 'Note', 'crossing:note', 'payment:notes', 'note:4', 'note:5', 'note:6', 'note:7', 'note:0', 'note:2', 'note:3', 'note:8', 'note:9', 'note:cycleway', 'FIXME:responce_1', 'note:12', 'cycleway:right:note', 'note:route_ref', 'note:healthcare', 'note 2', 'fee:note', 'note_3', 'note_2', 'note_1', 'cycleway:note', 'Fixme', 'fixme:license']\n"
     ]
    }
   ],
   "source": [
    "meta_keys = [key for key in unique_key_names if 'fixme' in key.lower() or 'note' in key.lower()]\n",
    "percentage = '%.2f' % (float(len(meta_keys)) / len(unique_key_names) * 100)\n",
    "print \"There are {} 'meta' tag key names making up {}% of all unique tag key names.\".format(len(meta_keys), percentage)\n",
    "print \"\\n\"\n",
    "print [key for key in unique_key_names if 'fixme' in key.lower() or 'note' in key.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Ideas about additional improvement , their benifits and problems :\n",
    "\n",
    "After this review of the data itâ€™s obvious that the area Oxford is incomplete, though I believe it has been well cleaned for the purposes of this exercise. However, the improvement may bring about potential problems if it's not implemented correctly:\n",
    "\n",
    "-  When we audit the data, it was very clear that although there are minor error caused by human input, the dataset is fairly well-cleaned. Considering there're hundreds of contributors for this map, there is a great numbers of human errors in this project. So, Gamifimation may impact the quality (veracity) of the data submitted from the contributors. We need to keep in mind that quality should always be considered more important than quantity when we try implementing the improvement.\n",
    "- If the difference between the highest score and the rest is too large, users may easily loose their interest. Therefore, we should implement it in such a way that the higher the score is, the harder it becomes to increase. \n",
    "- Since OpenStreetMaps is an open source project, there're still a lot of areas left unexplored as people tend to focus on a certain key areas and left other part outdated. we can resolve this issue by cross-referencing/cross-validating missing data from other database like Google API. Since each node has a coordinate (lattitude & longtitude), this process is definitely do-able.\n",
    "- With a rough GPS data processor in place and working together with a more robust data processor similar to data.py, I think it would be possible to input great amount of cleaned data to OpenStreetMap.org.\n",
    "- There we may face some potential cost of implementation. The amount of effort to engineer all these processes and the cost of creating, auditing & maintaining these initiatives could be so overwhelm and require a dedicated team responsible for all these projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The production of this project was aided by information found on the following website (various authors and contributors)\n",
    "\n",
    "- [Udacity.com](udacity.com)\n",
    "- [Docs.MongoDB.com](docs.mongodb.com)\n",
    "- [API.MongoDB.com/python](api.mongodb.com/python)\n",
    "- [StackOverflow.com](stackoverflow.com)\n",
    "- [Mapzen](https://mapzen.com/data/metro-extracts/)\n",
    "- Sample project [Github Repo](https://github.com/lyvinhhung/Udacity-Data-Analyst-Nanodegree/blob/master/p3%20-%20Wrangle%20OpenStreetMap%20Data/P3%20-%20Data%20Wrangling%20with%20MongoDB.ipynb)\n",
    "- [Blog Ref-1](http://ghunt03.github.io/DAProjects/DAP03/index.html)\n",
    "- [Blog Ref-2](https://olegleyz.github.io/data_wrangling.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
